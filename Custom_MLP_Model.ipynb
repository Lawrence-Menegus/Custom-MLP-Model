{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7f8cf0e0",
      "metadata": {
        "id": "7f8cf0e0"
      },
      "source": [
        "# Custom MLP Model for Flower Classification\n",
        "I created a Custom MultiLayer Perceptron (MLP) model using tensorflow Keras to classify images of flowers. I utilized data augmentation and ImageDataGenerator to preprocess the images, followed by training a custom MLP model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86797be0"
      },
      "source": [
        "## To Run in Google Colab"
      ],
      "id": "86797be0"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ba23279",
        "outputId": "bec2eaf1-85f0-4119-fc36-1adcdbf33d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " CNN_AutoEncoder.ipynb\t        flowers_train_validation\t\t    VAE.ipynb\n",
            " CNNS_Transfer_Learning.ipynb   flowers.zip\t\t\t\t    WGAN.ipynb\n",
            " Custom_MLP_Model.ipynb        'MultiLayer_Perceptron_(MLP) -Model.ipynb'\n",
            "Dataset unzipped successfully!\n",
            " CNN_AutoEncoder.ipynb\t        flowers_train_validation\t\t    VAE.ipynb\n",
            " CNNS_Transfer_Learning.ipynb   flowers.zip\t\t\t\t    WGAN.ipynb\n",
            " Custom_MLP_Model.ipynb        'MultiLayer_Perceptron_(MLP) -Model.ipynb'\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List MyDrive before t\n",
        "!ls /content/drive/MyDrive/\n",
        "\n",
        "\n",
        "\n",
        "# Open dataset from zipfile\n",
        "zip_path = \"/content/drive/MyDrive/flowers.zip\"\n",
        "extract_path = '/content/drive/MyDrive/'\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset unzipped successfully!\")\n",
        "\n",
        "\n",
        "\n",
        "# lists all files in Google Drive\n",
        "!ls /content/drive/MyDrive/"
      ],
      "id": "5ba23279"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bc4717d"
      },
      "source": [
        "## Run on Local Machine"
      ],
      "id": "6bc4717d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "ab540565",
        "outputId": "a4af088f-7201-4f64-c170-48564d2c320e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nimport os\\nimport zipfile\\n\\n# Define the paths for the zip file and extraction directory\\nzip_path = r\"C:\\\\path\\to\\\\your\\x0clowers.zip\"  # Update this to your zip file\\'s actual path\\nextract_path = r\"C:\\\\path\\to\\\\extract\\\\directory\"  # Update this to your desired extraction directory\\n\\n# Ensure the extraction path exists\\nos.makedirs(extract_path, exist_ok=True)\\n\\n# Extract the zip file\\ntry:\\n    with zipfile.ZipFile(zip_path, \\'r\\') as zip_ref:\\n        zip_ref.extractall(extract_path)\\n    print(\"Dataset unzipped successfully!\")\\nexcept FileNotFoundError:\\n    print(f\"Error: The file {zip_path} does not exist.\")\\nexcept zipfile.BadZipFile:\\n    print(f\"Error: The file {zip_path} is not a valid zip file.\")\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Define the paths for the zip file and extraction directory\n",
        "zip_path = r\"C:\\path\\to\\your\\flowers.zip\"  # Update this to your zip file's actual path\n",
        "extract_path = r\"C:\\path\\to\\extract\\directory\"  # Update this to your desired extraction directory\n",
        "\n",
        "# Ensure the extraction path exists\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Dataset unzipped successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {zip_path} does not exist.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: The file {zip_path} is not a valid zip file.\")\n",
        "\n",
        "'''\n"
      ],
      "id": "ab540565"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1be5c0e6-2fdc-480f-a301-fa4667722c00",
      "metadata": {
        "id": "1be5c0e6-2fdc-480f-a301-fa4667722c00"
      },
      "outputs": [],
      "source": [
        "# Google CoLab\n",
        "train_dir  = '/content/drive/MyDrive/flowers_train_validation/train'\n",
        "validation_dir = '/content/drive/MyDrive/flowers_train_validation/validation'\n",
        "\n",
        "# Local Machine\n",
        "#Train = 'C:/path to local drive for extracted files /flowers_train_validation/train'\n",
        "#Validation = 'C:/path to local drive for extracted files /flowers_train_validation/validation'\n",
        "TARGET_SIZE=(150,150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0aa8c293-be0d-45ef-a616-10e1a98462a0",
      "metadata": {
        "id": "0aa8c293-be0d-45ef-a616-10e1a98462a0"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow and Keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f26463",
      "metadata": {
        "id": "22f26463"
      },
      "source": [
        "## 1: Data Preparation and Augmentation\n",
        "We'll use the ImageDataGenerator class to augment our training data and rescale the images. Data augmentation helps in increasing the diversity of the training data, which helps in reducing overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eab76c9f-0612-4757-8929-7dde9e7d1683",
      "metadata": {
        "id": "eab76c9f-0612-4757-8929-7dde9e7d1683"
      },
      "outputs": [],
      "source": [
        "# Create ImageDataGenerators for training data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                    rotation_range=20,\n",
        "                                    width_shift_range=0.2,\n",
        "                                    height_shift_range=0.2,\n",
        "                                    shear_range=0.2,\n",
        "                                    zoom_range=0.2,\n",
        "                                    horizontal_flip=True,\n",
        "                                    fill_mode='nearest')\n",
        "\n",
        "# create validation generator with rescale, no augmentation\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "64b99aa0-770a-4e2e-9238-ae22cd84a96a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64b99aa0-770a-4e2e-9238-ae22cd84a96a",
        "outputId": "6f162805-e38b-472f-f642-4381fbd496f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3177 images belonging to 5 classes.\n",
            "Found 865 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,  # This is the target directory\n",
        "    target_size=(150, 150),  # All images will be resized to 150x150\n",
        "    batch_size=128,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4444c80e-b26a-4095-b8f7-b66fd7466343",
      "metadata": {
        "id": "4444c80e-b26a-4095-b8f7-b66fd7466343"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3815e35c",
      "metadata": {
        "id": "3815e35c"
      },
      "source": [
        "## 2: Building the Custom MLP Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10dcea6-9beb-4a55-a42a-f4f0131fdefa",
      "metadata": {
        "id": "f10dcea6-9beb-4a55-a42a-f4f0131fdefa"
      },
      "source": [
        "### 2.1 Custom Categorical Crossentropy Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5234a089-bf84-4556-8ecb-486489eb01e3",
      "metadata": {
        "id": "5234a089-bf84-4556-8ecb-486489eb01e3"
      },
      "source": [
        "**Defining Custom Categorical CrossEntropy Loss**\n",
        "\n",
        "The categorical cross-entropy formula is:\n",
        "$$L = - \\sum_{i=1}^{N} y_i \\log(\\hat{y_i})$$\n",
        "Where:\n",
        "- $N$ is the number of classes.\n",
        "- $y_i$ is the true label (one-hot encoded, 1 for the correct class, and 0 for the others).\n",
        "- $\\hat{y_i}$ is the predicted probability for the corresponding class (output of a `softmax`).\n",
        "\n",
        "For each sample\n",
        "- Multiply with One-Hot Labels: Multiply the logarithm of the predictions with the corresponding one-hot encoded labels (y_true), so only the correct class’s prediction contributes to the loss.\n",
        "- Sum the Results: Sum the result of the above operation across all classes for each sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "06916529-4a3a-4a2a-b8ae-d7bac531a050",
      "metadata": {
        "id": "06916529-4a3a-4a2a-b8ae-d7bac531a050"
      },
      "outputs": [],
      "source": [
        "# Custom categorical entropy loss function\n",
        "def my_categorical_crossentropy(y_true, y_pred):\n",
        "\n",
        "    # Clip prediction values to avoid log(0) error\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-10, 1.0)\n",
        "\n",
        "    # Compute the categorical cross-entropy loss\n",
        "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
        "\n",
        "    # Return the mean loss over the batch\n",
        "    loss = tf.reduce_mean(loss)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b8722e09-26ae-4619-be73-2529065cc8d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8722e09-26ae-4619-be73-2529065cc8d0",
        "outputId": "0fa2faed-8b69-4dc5-b82f-ac760f5f1a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom categorical entropy loss:  tf.Tensor(0.31903753, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Define y_true (one-hot encoded labels) and y_pred (predicted probabilities)\n",
        "y_true = np.array([\n",
        "    [1, 0, 0, 0, 0],  # Class 0\n",
        "    [0, 1, 0, 0, 0],  # Class 1\n",
        "    [0, 0, 0, 1, 0]   # Class 3\n",
        "])\n",
        "\n",
        "y_pred = np.array([\n",
        "    [0.8, 0.0, 0.1, 0.05, 0.05],  # Model is confident about class 0\n",
        "    [0.1, 0.6, 0.1, 0.1, 0.1],    # Model is confident about class 1\n",
        "    [0.05, 0.05, 0.05, 0.8, 0.05]  # Model is confident about class 3\n",
        "])\n",
        "\n",
        "# Convert y_true and y_pred to tensors\n",
        "y_true_tensor = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
        "y_pred_tensor = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
        "\n",
        "# Compute the custom loss\n",
        "loss = my_categorical_crossentropy(y_true_tensor, y_pred_tensor)\n",
        "\n",
        "# Run the computation in a TensorFlow session\n",
        "print(\"Custom categorical entropy loss: \", loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a859e923-e025-42f8-8bd6-cf555efb3ecf",
      "metadata": {
        "id": "a859e923-e025-42f8-8bd6-cf555efb3ecf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "508632c8-af34-41a3-91b1-36e4b61f4531",
      "metadata": {
        "id": "508632c8-af34-41a3-91b1-36e4b61f4531"
      },
      "source": [
        "### 2.2 Custom Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e023d8-78c1-4b38-85e2-d623681b7a5e",
      "metadata": {
        "id": "14e023d8-78c1-4b38-85e2-d623681b7a5e"
      },
      "source": [
        "### 2.2.1 MyFlatten Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6157a812-e0ad-468d-ba03-42e6a98fefe6",
      "metadata": {
        "id": "6157a812-e0ad-468d-ba03-42e6a98fefe6"
      },
      "outputs": [],
      "source": [
        "# Custom Flatten layer\n",
        "class MyFlatten(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(MyFlatten, self).__init__()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Flatten the input\n",
        "        return tf.reshape(inputs, [inputs.shape[0], -1])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # The output shape is (batch_size, flattened_dims)\n",
        "        # Calculate the product of all dimensions except the batch size (input_shape[0])\n",
        "        flatten_dim = 1\n",
        "        for dim in input_shape[1:]:\n",
        "            if dim is not None:\n",
        "                flatten_dim *= dim\n",
        "            else:\n",
        "                # If any dimension is None, return None (because we cannot calculate the product statically)\n",
        "                return (input_shape[0], None)\n",
        "\n",
        "        return (input_shape[0], flatten_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "813a0e17-b0a6-4026-a493-74a9faafc8e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "813a0e17-b0a6-4026-a493-74a9faafc8e1",
        "outputId": "7bf897cf-f5d9-4ac3-cd28-3e885df829dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (1, 3, 3)\n",
            "Flattened output: [[1. 2. 3. 4. 5. 6. 7. 8. 9.]]\n",
            "Flattened output shape: (1, 9)\n"
          ]
        }
      ],
      "source": [
        "# Example of input with size (3, 3)\n",
        "input_data = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=tf.float32)\n",
        "\n",
        "# Reshape input_data to add batch dimension (batch_size, 3, 3)\n",
        "input_data = tf.expand_dims(input_data, axis=0)  # Adding batch size of 1, so shape is (1, 3, 3)\n",
        "\n",
        "# Instantiate and apply the custom flatten layer\n",
        "flatten_layer = MyFlatten()\n",
        "flattened_output = flatten_layer(input_data)\n",
        "\n",
        "# Print the result\n",
        "print(\"Input shape:\", input_data.shape)\n",
        "print(\"Flattened output:\", flattened_output.numpy())\n",
        "print(\"Flattened output shape:\", flattened_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff34ea7-491e-4a5d-8ea3-5687cbc0df6c",
      "metadata": {
        "id": "9ff34ea7-491e-4a5d-8ea3-5687cbc0df6c"
      },
      "source": [
        "### 2.2.2 MyDense Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cbe96d0-9586-49c5-acd2-c11541752616",
      "metadata": {
        "id": "1cbe96d0-9586-49c5-acd2-c11541752616"
      },
      "source": [
        "**Defining Custom Dense Layer supporting activation**:\n",
        "\n",
        "- **init()**: initialize units and activations\n",
        "- **build()**: initialize weights and biases\n",
        "- **call()**: forward pass\n",
        "- **compute_output_shape()**: define output shape of the layer. It is always (batch_size, units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bc52aa2f-9d34-45ae-9ba0-0e98ed9da29b",
      "metadata": {
        "id": "bc52aa2f-9d34-45ae-9ba0-0e98ed9da29b"
      },
      "outputs": [],
      "source": [
        "class MyDense(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units=32, activation=None):\n",
        "        super(MyDense, self).__init__()\n",
        "        self.units = units\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(shape=(self.units,),\n",
        "                                 initializer='zeros',\n",
        "                                 trainable=True)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.activation(tf.matmul(inputs, self.w) + self.b)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.units)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7850ff4-e089-48bd-a1b6-0908877ed4d3",
      "metadata": {
        "id": "f7850ff4-e089-48bd-a1b6-0908877ed4d3"
      },
      "source": [
        "### 2.3 Custom Model using Subclassing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65aa3145-961f-4704-bcee-04e40eecb270",
      "metadata": {
        "id": "65aa3145-961f-4704-bcee-04e40eecb270"
      },
      "source": [
        "**Defining Custom Model for Flower Prediction**:\n",
        "\n",
        "- **init()**: create instances of layers\n",
        "- **call()**: forward pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "68e8ea53-c726-4342-93f2-cf2c25a59add",
      "metadata": {
        "id": "68e8ea53-c726-4342-93f2-cf2c25a59add"
      },
      "outputs": [],
      "source": [
        "# Custom model class\n",
        "class MyFlowerModel(tf.keras.models.Model):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MyFlowerModel, self).__init__()\n",
        "        self.flatten = MyFlatten()\n",
        "        self.dense1 = MyDense(200, activation='relu')\n",
        "        self.dense2 = MyDense(150, activation='relu')\n",
        "        self.dense3 = MyDense(num_classes, activation='softmax')\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.flatten(inputs)\n",
        "        x = self.dense1(x)\n",
        "        x= self.dense2(x)\n",
        "        return self.dense3(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fd036a70-5574-4943-8543-4ff83351cb50",
      "metadata": {
        "id": "fd036a70-5574-4943-8543-4ff83351cb50"
      },
      "outputs": [],
      "source": [
        "model = MyFlowerModel(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "880c2a92-90a8-4a89-8d2a-e793ca3d8127",
      "metadata": {
        "id": "880c2a92-90a8-4a89-8d2a-e793ca3d8127"
      },
      "source": [
        "## 3. Custom Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11ca5f1e-7345-429a-bc46-94abc2519c10",
      "metadata": {
        "id": "11ca5f1e-7345-429a-bc46-94abc2519c10"
      },
      "source": [
        "### 3.1 Create instances for Optimizer and Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c34279c1-5b81-49f9-bf9e-824b3727f4cc",
      "metadata": {
        "id": "c34279c1-5b81-49f9-bf9e-824b3727f4cc"
      },
      "source": [
        "**Create instances for optimizer and loss**\n",
        "\n",
        "- `adam` optimizer and\n",
        "- custom categorical crossentropy loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d0250a53-27e1-46a4-a135-63df0e7ff133",
      "metadata": {
        "id": "d0250a53-27e1-46a4-a135-63df0e7ff133"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = my_categorical_crossentropy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53793419-dab8-4451-b64d-c77961a0d3be",
      "metadata": {
        "id": "53793419-dab8-4451-b64d-c77961a0d3be"
      },
      "source": [
        "### 3.2 Define Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea393ee-620b-49f8-b7b1-a93573b5d60d",
      "metadata": {
        "id": "3ea393ee-620b-49f8-b7b1-a93573b5d60d"
      },
      "source": [
        "**Create instances for metrics (both train and validation)**\n",
        "\n",
        "- Using `CategoricalAccuracy`defined in `tf.keras.metrics`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "206e2ff3-4e37-4986-8596-455fe4118a35",
      "metadata": {
        "id": "206e2ff3-4e37-4986-8596-455fe4118a35"
      },
      "outputs": [],
      "source": [
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6177249f-64e8-478a-9e34-80a3c7dbca6d",
      "metadata": {
        "id": "6177249f-64e8-478a-9e34-80a3c7dbca6d"
      },
      "source": [
        "### 3.3 Custom Training Loop\n",
        "\n",
        "The core of training is using the model to calculate the logits on specific set of inputs and compute loss (in this case **categorical crossentropy**) by comparing the predicted outputs to the true outputs. Update the trainable weights using the optimizer algorithm chosen. Optimizer algorithm requires the computed loss and partial derivatives of loss with respect to each of the trainable weights to make updates to the same."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83851422-c634-4748-8401-360fe6b0132a",
      "metadata": {
        "id": "83851422-c634-4748-8401-360fe6b0132a"
      },
      "source": [
        "#### 3.3.1. Gradient Calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04cb3f3a-3b24-44ea-8cbe-d60b6b289a92",
      "metadata": {
        "id": "04cb3f3a-3b24-44ea-8cbe-d60b6b289a92"
      },
      "source": [
        "**Apply gradients on optimizer**\n",
        "\n",
        "- *optimizer*: your optimizer used to optimize the model paramenters\n",
        "- *model*: your custom flower model\n",
        "- *x*: input training x\n",
        "- *y*: input training y\n",
        "\n",
        "The function will use tensorflow's gradientTape to calculate the gradients and then optimize the parameters through optimizer. The function will return logits (model's predicted values) and loss_value (calculated by the loss function).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0918a1ca-46ec-4dab-9d0d-aff690675ff6",
      "metadata": {
        "id": "0918a1ca-46ec-4dab-9d0d-aff690675ff6"
      },
      "outputs": [],
      "source": [
        "def apply_gradient(optimizer, model, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x)\n",
        "        loss_value = loss_object(y_true=y, y_pred=logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return logits, loss_value\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d99e229-8823-4a69-9905-9c1c1fe3c639",
      "metadata": {
        "id": "1d99e229-8823-4a69-9905-9c1c1fe3c639"
      },
      "source": [
        "### 3.3.2 Define a Training for Each Epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84c22ad5-c67d-4f86-94fd-efbfb1c34747",
      "metadata": {
        "id": "84c22ad5-c67d-4f86-94fd-efbfb1c34747"
      },
      "source": [
        "**Task 7: train_data_for_one_epoch()**\n",
        "\n",
        "This function performs training during one epoch. You run through all batches of training data in each epoch to make updates to trainable weights using the previous function. It will call update_state on your metrics to accumulate the value of the metrics.\n",
        "Display a progress bar to indicate completion of training in each epoch (use **tqdm** for displaying the progress bar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a92510b7-5120-4da8-b2be-1f6d65e5287f",
      "metadata": {
        "id": "a92510b7-5120-4da8-b2be-1f6d65e5287f"
      },
      "outputs": [],
      "source": [
        "def train_data_for_one_epoch():\n",
        "    losses = []\n",
        "    steps = train_generator.samples // train_generator.batch_size\n",
        "    pbar = tqdm(total=steps, position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_generator):\n",
        "        if step >= steps - 1:\n",
        "            break\n",
        "\n",
        "        # Call the function to apply gradient and get logits and loss\n",
        "        logits, loss_value = apply_gradient(optimizer, model, x_batch_train, y_batch_train)\n",
        "\n",
        "        losses.append(loss_value)\n",
        "\n",
        "        # Update state\n",
        "        train_acc_metric(y_batch_train, logits)\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_description(\"Training loss for step %s: %.4f\" % (int(step), float(loss_value)))\n",
        "        pbar.update()\n",
        "\n",
        "    return losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02dbd1cf-b2c4-4fc8-8c20-9585e68f4af3",
      "metadata": {
        "id": "02dbd1cf-b2c4-4fc8-8c20-9585e68f4af3"
      },
      "source": [
        "#### 3.3.3 Perform Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c829d450-1d78-45ab-9cc1-9c503d369c49",
      "metadata": {
        "id": "c829d450-1d78-45ab-9cc1-9c503d369c49"
      },
      "source": [
        "**perform_validation()**\n",
        "\n",
        "- Change **enumerate(test)** to  **enumerate(validation_generator)**, due to the fact we use **imageDataGenerator** not **tfds**\n",
        "- Add **STEPS= validation_generator.samples // validation_generator.batch_size** before the loop (needed for stopping the generator)\n",
        "- Stop the loop after reaching the number of STEPS. i.e. add the statements at the end in the loop: **if step >= STEPs - 1: break**.  (*Note*: same logic as above.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "606a92c4-a689-42e2-aca0-ef1b836ed387",
      "metadata": {
        "id": "606a92c4-a689-42e2-aca0-ef1b836ed387"
      },
      "outputs": [],
      "source": [
        "def perform_validation():\n",
        "    losses = []\n",
        "\n",
        "    # Calculate the number of steps\n",
        "    STEPS = validation_generator.samples // validation_generator.batch_size\n",
        "\n",
        "    for step, (x_val, y_val) in enumerate(validation_generator):\n",
        "        val_logits = model(x_val)\n",
        "        val_loss = loss_object(y_true=y_val, y_pred=val_logits)\n",
        "        losses.append(val_loss)\n",
        "\n",
        "        # Update metrics\n",
        "        val_acc_metric(y_val, val_logits)\n",
        "\n",
        "        # Stop after reaching the number of STEPS\n",
        "        if step >= STEPS - 1:\n",
        "            break\n",
        "\n",
        "    return losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "52fe82b4-ef74-4805-bc18-4e39aca4ea82",
      "metadata": {
        "id": "52fe82b4-ef74-4805-bc18-4e39aca4ea82"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1701eb15-0c0c-4dc3-b338-2e77c5296768",
      "metadata": {
        "id": "1701eb15-0c0c-4dc3-b338-2e77c5296768"
      },
      "source": [
        "### 3.3.4 Model fit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a44fb1ad-c419-4db7-a1f1-ba71ea99704f",
      "metadata": {
        "id": "a44fb1ad-c419-4db7-a1f1-ba71ea99704f"
      },
      "source": [
        "**Perform model fit using training Loops**\n",
        "\n",
        "1. Perform training over all batches of training data.\n",
        "2. Get values of metrics.\n",
        "3. Perform validation to calculate loss and update validation metrics on test data.\n",
        "4. Reset the metrics at the end of epoch.\n",
        "5. Display statistics at the end of each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "acda3eec-e22f-4b33-b9fe-6f2be469e334",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acda3eec-e22f-4b33-b9fe-6f2be469e334",
        "outputId": "169f9eb0-042b-46cd-e65a-e9f654aee59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loss for step 22: 11.1043:  96%|█████████▌| 23/24 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1: Train loss: 11.5035  Validation Loss: 10.1205, Train Accuracy: 0.2303, Validation Accuracy 0.2812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loss for step 22: 4.2654:  96%|█████████▌| 23/24 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 2: Train loss: 8.5405  Validation Loss: 3.3775, Train Accuracy: 0.2711, Validation Accuracy 0.3517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loss for step 22: 1.5262:  96%|█████████▌| 23/24 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 3: Train loss: 1.8819  Validation Loss: 1.4628, Train Accuracy: 0.3016, Validation Accuracy 0.3457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loss for step 22: 1.3543:  96%|█████████▌| 23/24 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 4: Train loss: 1.3952  Validation Loss: 1.3497, Train Accuracy: 0.3916, Validation Accuracy 0.4070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loss for step 22: 1.2981:  96%|█████████▌| 23/24 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 5: Train loss: 1.3162  Validation Loss: 1.3409, Train Accuracy: 0.4296, Validation Accuracy 0.4250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loss for step 22: 1.2403:  96%|█████████▌| 23/24 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 6: Train loss: 1.3002  Validation Loss: 1.2780, Train Accuracy: 0.4553, Validation Accuracy 0.4514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loss for step 22: 1.2776:  96%|█████████▌| 23/24 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 7: Train loss: 1.2845  Validation Loss: 1.2916, Train Accuracy: 0.4629, Validation Accuracy 0.4490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loss for step 22: 1.2310:  96%|█████████▌| 23/24 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 8: Train loss: 1.3132  Validation Loss: 1.2608, Train Accuracy: 0.4296, Validation Accuracy 0.4286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loss for step 22: 1.3031:  96%|█████████▌| 23/24 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 9: Train loss: 1.2888  Validation Loss: 1.2673, Train Accuracy: 0.4608, Validation Accuracy 0.4610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training loss for step 22: 1.1899:  96%|█████████▌| 23/24 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 10: Train loss: 1.2790  Validation Loss: 1.3452, Train Accuracy: 0.4399, Validation Accuracy 0.4298\n"
          ]
        }
      ],
      "source": [
        "# your model fitting\n",
        "epochs = 10\n",
        "epochs_val_losses, epochs_train_losses = [], []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    losses_train = train_data_for_one_epoch()\n",
        "    train_acc = train_acc_metric.result()\n",
        "\n",
        "    losses_val = perform_validation()\n",
        "    val_acc = val_acc_metric.result()\n",
        "\n",
        "    losses_train_mean = np.mean(losses_train)\n",
        "    losses_val_mean = np.mean(losses_val)\n",
        "    epochs_val_losses.append(losses_val_mean)\n",
        "    epochs_train_losses.append(losses_train_mean)\n",
        "\n",
        "    print('\\n Epoch %s: Train loss: %.4f  Validation Loss: %.4f, Train Accuracy: %.4f, Validation Accuracy %.4f' % (epoch + 1, float(losses_train_mean), float(losses_val_mean), float(train_acc), float(val_acc)))\n",
        "\n",
        "    train_acc_metric.reset_state()\n",
        "    val_acc_metric.reset_state()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "216cbf95-d9bb-46a4-98e6-b98f45478319",
      "metadata": {
        "id": "216cbf95-d9bb-46a4-98e6-b98f45478319"
      },
      "source": [
        "**Model Summary**\n",
        "- Layer types (MyFlatten, not Keras.Flatten, MyDense, not keras.Dense)\n",
        "- Output shapes (batch size, units)\n",
        "- Parameters (based on fully connected neurons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "efd16d3f-9cdb-4564-8b85-3a71e61eea6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efd16d3f-9cdb-4564-8b85-3a71e61eea6e",
        "outputId": "53acf9f7-1951-4e99-c3cb-c8d02a121f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_flower_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " my_flatten_1 (MyFlatten)    multiple                  0         \n",
            "                                                                 \n",
            " my_dense (MyDense)          multiple                  13500200  \n",
            "                                                                 \n",
            " my_dense_1 (MyDense)        multiple                  30150     \n",
            "                                                                 \n",
            " my_dense_2 (MyDense)        multiple                  755       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13531105 (51.62 MB)\n",
            "Trainable params: 13531105 (51.62 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# your model summary\n",
        "model.summary()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}